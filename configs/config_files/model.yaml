MODEL:
  CONTEXT_WINDOW_SIZE: 4096
  N_GPU_LAYERS: 40  # Llama-2-70B has 83 layers
  N_BATCH: 512
  DEVICE: 'cuda' # ["cpu", "cuda", "ipu", "xpu", "mkldnn", "opengl", "opencl", "ideep", "hip", 
                #  "ve", "fpga", "ort", "xla", "lazy", "vulkan", "mps", "meta", "hpu", "mtia"]

  SIMILARITY_THRESHOLD: 0.7
  TEMPERATURE: 0.2

  USE_HISTORY: True
  USE_RETRIEVER: True
  USE_TRANSLATE: False
  USE_OLLAMA: True

  # SYSTEM_PROMPT: "
  #   Bạn là một trợ lý ảo hữu ích, bạn sẽ sử dụng ngữ cảnh được cung cấp để trả lời các câu hỏi của người dùng. 
  #   Hãy đọc ngữ cảnh được cung cấp trước khi trả lời câu hỏi và suy nghĩ từng bước.
  # "

  SYSTEM_PROMPT: " 
    Bạn là một trợ lý ảo hữu ích, bạn sẽ sử dụng ngữ cảnh được cung cấp để trả lời các câu hỏi của người dùng. 
    Bạn hãy giúp người người dùng bằng cách đọc ngữ cảnh được cung cấp, suy nghĩ thật kỹ câu hỏi và trả lời chi tiết từng bước.
  "


  MODEL_TYPE: "mistral" # ["llama", "mistral", "non_llama"],
  
  # https://huggingface.co/spaces/mteb/leaderboard
  EMBEDDING_MODEL_NAME: "intfloat/multilingual-e5-large" #=> 51 #"Salesforce/SFR-Embedding-Mistral" =>58 # "intfloat/e5-mistral-7b-instruct" => 56.89

  
  # MODEL_ID: "vilm/vinallama-7b-chat-GGUF"
  # MODEL_BASENAME: "vinallama-7b-chat_q5_0.gguf"
  
  # MODEL_ID: "mistralai/Mistral-7B-Instruct-v0.2"
  # MODEL_BASENAME: ""
  
  MODEL_ID: "ontocord/vistral" # "Qwen/Qwen1.5-7B-Chat-AWQ" # vistral #mistral  # qwen14B
  MODEL_BASENAME: ""

  # MODEL_ID: "qnguyen3/quan-1.8b-chat"
  # MODEL_BASENAME: ""

  # MODEL_ID: "Viet-Mistral/Vistral-7B-Chat"
  # MODEL_BASENAME: ""

  #MODEL_ID: "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF"
  #MODEL_BASENAME: "tinyllama-1.1b-chat-v0.3.Q6_K.gguf"

  # MODEL_ID: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  # MODEL_BASENAME: ""

    